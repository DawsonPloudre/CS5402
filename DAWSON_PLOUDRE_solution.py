# Author: Dawson Ploudre
# Email: dcpfmb@umsystem.edu
import pandas as pd
import ast
import numpy as np
import wfdb
from typing import Dict

# Step-1, part 1
def parse_ptbxl_data() -> pd.DataFrame:
    """
    Use pandas for this task.
    Implement this function to parse the ptbxl samples. It should
    return a dataframe with the filename_lr and diagnostic_class column.
    The filename_lr is the filename of the low-resolution ECG signal, and
    the diagnostic_class is generated by converting the scp_codes
    in the ptbxl database to diagnostic_class in the scp_statements database.
    There are 5 classes in total: [NORM, MI, STTC, CD, HYP].
    
    Since we want to simulate using the dataset to train a model, the class needs to have 
    at least one class, and any empty entries should be removed. 
    
    The end result should look like this:
    ecg_id filename_lr diagnostic_class
    1 records100/xxxxx/xxxxxx_lr  ['HYP']
    2 records100/xxxxx/xxxxxx_lr  ['MI']
    3 records100/xxxxx/xxxxxx_lr  ['MI, STTC']
    ...
    """
    ptbxl_df = pd.read_csv('ptbxl_sample.csv')
    scp_df = pd.read_csv('scp_statements.csv', index_col=0)
    
    # Most of this stuff is self explanatory, not too hard
    def map_diagnostic(scp_codes_str):
        scp_codes = ast.literal_eval(scp_codes_str)
        classes = set()
        for code in scp_codes:
            if code in scp_df.index:
                classes.add(scp_df.loc[code, 'diagnostic_class'])
        return list(classes)
    
    ptbxl_df['diagnostic_class'] = ptbxl_df['scp_codes'].apply(map_diagnostic)
    
    ptbxl_df = ptbxl_df[ptbxl_df['diagnostic_class'].map(len) > 0]
    
    return ptbxl_df[['filename_lr', 'diagnostic_class']]

# Step-1, part 2
def create_dataset(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray]:
    """
    Use numpy and wfdb for this task.
    The data are located in /records100/, and you can read them like this:
    signal, _ = wfdb.rdsamp(filepath), where signal is a numpy array that
    should have shape [signal_length(1000), num_channels(12)].
    Implement this function to create a dataset from the dataframe df, which should be the output of 
    parse_ptbxl_data. 

    Convert the textual class labels into one-hot encoding. For example, using the label order [NORM, MI, STTC, CD, HYP],
    an ECG signal with labels [HYP, MI, STTC] would be converted to [0, 1, 1, 0, 1].

    Return two numpy arrays:
    - data_x: array should contain the ECG data with shape [num_samples, signal_length(1000), num_channels(12)].
    - data_y: array should contain the labels with shape   [num_samples, num_classes(5)].
    """
    label_order = ['NORM', 'MI', 'STTC', 'CD', 'HYP']
    data_x, data_y = [], []
    
    # Make it only 300 samples???
    df = df.iloc[:300]
    
    for _, row in df.iterrows():
        signal, _ = wfdb.rdsamp(row['filename_lr'])
        data_x.append(signal)
        labels = np.zeros(len(label_order))
        for label in row['diagnostic_class']:
            if label in label_order:
                labels[label_order.index(label)] = 1
        data_y.append(labels)
    
    return np.array(data_x), np.array(data_y)

# Step-2
def data_preprocessing(data_x: np.ndarray, data_y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    """
    Perform data preprocessing: 
    - Check for missing values (or N/A), anomalies, and outliers. 
        - Fill missing values with the average of adjacent points in the same channel. 
        - Replace outliers using the 97th percentile (np.percentile(x, 97)). 
    - Normalize each channel with the equation: (x - xmin)/(xmax - xmin).
        - xmax: represents the maximum value of a channel
        - xmin: represents the minimum value of the channel.
    After normalization, the values will be scaled to range from 0 to 1.
    """
    X_processed = np.copy(data_x)
    
    # This should fill in missing values hopefully
    for i in range(X_processed.shape[0]):
        for j in range(X_processed.shape[2]):
            channel = X_processed[i, :, j]
            nan_indices = np.isnan(channel)
            if np.any(nan_indices):
                non_nan_indices = ~nan_indices
                if np.any(non_nan_indices):
                    valid_indices = np.where(non_nan_indices)[0]
                    nan_positions = np.where(nan_indices)[0]
                    channel[nan_indices] = np.interp(
                        nan_positions,
                        valid_indices,
                        channel[non_nan_indices]
                    )
                else:
                    channel[nan_indices] = 0
    
    # Handle outliers, this is a bit weird but it works
    for j in range(X_processed.shape[2]):
        channel = X_processed[:, :, j]
        lower = np.percentile(channel, 3)
        upper = np.percentile(channel, 97)
        channel[channel < lower] = lower
        channel[channel > upper] = upper
    
    data_x_normalized = np.zeros_like(X_processed)
    for j in range(X_processed.shape[2]):
        channel = X_processed[:, :, j]
        xmin, xmax = np.min(channel), np.max(channel)
        if xmax > xmin:
            data_x_normalized[:, :, j] = (channel - xmin) / (xmax - xmin)
        else:
            data_x_normalized[:, :, j] = 0
    
    return data_x_normalized, data_y

def split_data(data_x: np.ndarray, data_y: np.ndarray) -> tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray]]:
    """
    Split the dataset into train, test, and validation sets at a 7:2:1 ratio.
    """
    X_processed, y_processed = data_preprocessing(data_x, data_y)
    
    num_samples = len(X_processed)
    train_size = int(0.7 * num_samples)
    val_size = int(0.2 * num_samples)
    
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    
    train_idx = indices[:train_size]
    val_idx = indices[train_size:train_size + val_size]
    test_idx = indices[train_size + val_size:]
    
    train_dataset = {"data_x": X_processed[train_idx], "data_y": y_processed[train_idx]}
    val_dataset = {"data_x": X_processed[val_idx], "data_y": y_processed[val_idx]}
    test_dataset = {"data_x": X_processed[test_idx], "data_y": y_processed[test_idx]}
    
    return train_dataset, val_dataset, test_dataset
